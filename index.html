<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="active inference, visual attention, Posner cueing task">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>An Active Inference Model of Covert and Overt Visual Attention</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">An Active Inference Model of Covert and Overt Visual Attention</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://www.fer.unizg.hr/en/tin.misic" target="_blank">Tin Mišić</a>,</span>
                <span class="author-block">
                  <a href="https://www.fer.unizg.hr/en/karlo.koledic" target="_blank">Karlo Koledić</a>,</span>
                  <span class="author-block">
                    <a href="https://www.fer.unizg.hr/en/fabio.bonsignorio" target="_blank">Fabio Bonsignorio</a>,
                  </span>
                  <span class="author-block">
                    <a href="https://www.fer.unizg.hr/en/ivan.petrovic" target="_blank">Ivan Petrović</a>,
                  </span>
                  <span class="author-block">
                    <a href="https://www.fer.unizg.hr/en/ivan.markovic" target="_blank">Ivan Marković</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">University of Zagreb, Faculty of Electrical Engineering and Computing<br>conference</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>-->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/unizgfer-lamor/ainf-visual-attention" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!--<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section>-->
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The ability to selectively attend to relevant stimuli while filtering out distractions is essential for agents that process complex, high-dimensional sensory input. This paper introduces a model of covert and overt visual attention through the framework of active inference, utilizing dynamic optimization of sensory precisions to minimize free-energy. The model determines visual sensory precisions based on both current environmental beliefs and sensory input, influencing attentional allocation in both covert and overt modalities. To test the effectiveness of the model, we analyze its behavior in the Posner cueing task and a simple target focus task using two-dimensional(2D) visual data. Reaction times are measured to investigate the interplay between exogenous and endogenous attention, as well as valid and invalid cueing. The results show that exogenous and valid cues generally lead to faster reaction times compared to endogenous and invalid cues. Furthermore, the model exhibits behavior similar to inhibition of return, where previously attended locations become suppressed after a specific cue-target onset asynchrony interval. Lastly, we investigate different aspects of overt attention and show that involuntary, reflexive saccades occur faster than intentional ones, but at the expense of adaptability.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<!--<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <img src="static/images/carousel1.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          First image description.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/carousel2.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Second image description.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/carousel3.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Third image description.
       </h2>
     </div>
     <div class="item">
      <img src="static/images/carousel4.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Fourth image description.
      </h2>
    </div>
  </div>
</div>
</div>
</section>-->
<!-- End image carousel -->




<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Video Examples</h2>
      <div class="columns">
        <div class="column is-four-fifths">
          <div class="content has-text-justified">
            <p>
              The following videos demonstrate the model's behaviour in the Posner cueing task and the Overt attention task. The visual sensory input is on the left, and the visual prediction with target (red arrow) and covert focus (green dot) beliefs is on the right.
            </p>
          </div>
          <h2 class="title is-4">The Posner cueing task</h2>
          <h2 class="title is-5">Endogenous cueing</h2>
          <div class="content has-text-justified">
            <p>
              The model is cued endogenously for 50 steps, and after a cue-target onset asynchrony period of 100 steps the target is shown.
            </p>
          </div>

          <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe src="https://www.youtube.com/embed/0CwjfhYo5eQ?si=ZPXYmipHABjOFr_k" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
      
          <h2 class="title is-5">Exogenous cueing</h2>
          <div class="content has-text-justified">
            <p>
              The model is cued exogenously for 50 steps, and after a cue-target onset asynchrony period of 100 steps the target is shown.
            </p>
          </div>

          <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe src="https://www.youtube.com/embed/KfKeA_bZ7O0?si=wMffTdiBdPnscePg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>

          <h2 class="title is-4">Overt attention - action enabled</h2>

          <div class="content has-text-justified">
            <p>
              The model is cued is presented a static object and moves the camera to focus it in the center of the image.
            </p>
          </div>

          <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe src="https://www.youtube.com/embed/Ay33udKygOo?si=TrSUHusLSf8WNw4p" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Simulation</h2>
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <h3>Active inference agent demo</h3>

          <p>To begin a simple instance of the active inference agent run the following 4 commands in separate terminals:</p>

          <ol>
            <li><code>ros2 launch gazebo_ros gazebo.launch.py world:=worlds/static_test.world</code></li>
            <li><code>ros2 run camera_orientation turn_cam</code></li>
            <li><code>ros2 topic pub /needs std_msgs/msg/Float32MultiArray "{data: [0.0, 0.0,1.0]}"</code> Note: edit the first two elements for the cue position, and the third for cue strength</li>
            <li><code>ros2 run aif_model act_inf</code></li>
          </ol>

          <p>Usage:<br/>
              - <strong>Enter</strong> advances the simulation by one step<br/> 
              - <strong>s</strong> sets the automatic step counter and advances the simulation by the given steps<br/>
              - <strong>c</strong> runs the simulation for the set step count<br/>
          </p>

          <h3>Automatic trials for tasks</h3>

          <p>To start the auto trial node for the Posner paradigm, or the overt attention trial, run the following 3 commands in separate terminals:</p>

          <ol>
            <li><code>ros2 launch gazebo_ros gazebo.launch.py world:=worlds/static_test.world</code></li>
            <li><code>ros2 run camera_orientation turn_cam</code></li>
            <li><code>ros2 run aif_model auto_trial</code></li>
          </ol>

          <p>You can edit the following arguments for the last command:<br/>
              - <strong><em>trials</em></strong>: number of trials. Default is 1<br/>
              - <strong><em>init</em></strong>: step duration of the initialization phase. Default is 10<br/>
              - <strong><em>cue</em></strong>: step duration for the cue phase. Default is 50<br/>
              - <strong><em>coa</em></strong>: step duration for the cue-target onset asynchrony phase. Default is 100<br/>
              - <strong><em>max</em></strong>: maximum number of simulation steps after target onset. Default is 1000<br/>
              - <strong><em>endo</em></strong>: boolean value indicating if trial is endogenous. Default is True, set False for exogenous<br/>
              - <strong><em>valid</em></strong>: boolean value indicating if trial is valid. Default is True, set False for invalid<br/>
              - <strong><em>act</em></strong>: boolean value indicating if action is enabled in the trial. Default is False, set True for overt attention<br/>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3">Requirements</h2>
    
    <ul>
      <li>ROS2 Humble Hawksbill</li>
      <li>Python 3.10.xx</li>
      <li>Numpy 1.26.4</li>
      <li>OpenCV 4.5.4</li>
      <li>Gazebo Simulator 11.10.2</li>
    </ul>
    </div>
  </div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop content">
    

  </div>
</section>

<!--Acknowledgement -->
<section class="section" id="acknowledgement">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgement</h2>
    <p>This research has been supported by the H2020 project AIFORS under Grant Agreement No 952275</p>
  </div>
</section>
<!--End acknowledgement citation -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->




  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
