"""Majority of code is by pi-tau on github: https://github.com/pi-tau/vae"""

import torch
import torch.nn as nn
import aif_model.config as c
import numpy as np
import aif_model.utils

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

class VAE(nn.Module):
    # Author: pi-tau
    """
    Variational Autoencoder as described in https://arxiv.org/abs/1312.6114

    The variational autoencoder models the latent space with an underlying
    standard Gaussian distribution. The data space is also modelled using a
    normal distribution with mean and variance generated by the decoder network.
    """

    def __init__(self, latent_dim, encoder, decoder):
        """
        Init a Variational Autoencoder model.

        Args:
            latent_dim: int
            encoder: nn.Module
            decoder: nn.Module
        """
        super().__init__()
        self.latent_dim = latent_dim
        self.encoder = encoder
        self.decoder = decoder

        self.register_buffer("mu_prior", torch.tensor(0.))
        self.register_buffer("std_prior", torch.tensor(1.))

    def prior(self):
        """
        Prior distribution
        """
        return torch.distributions.Normal(self.mu_prior, self.std_prior)
    
    def reparametrize(self, mu, log_var):
        """
        Reparametrization of encoded space
        """
        eps = self.prior().sample(mu.shape)
        return mu + log_var.exp() * eps

    def loss(self, x, y):
        """
        Compute the variational lower bound loss.
        """
        mu_z, log_std_z = self.encoder(x)
        z = self.reparametrize(mu_z, log_std_z)
        mu_x, log_std_x = self.decoder(z)

        recon_loss = 0.5 * (np.log(2 * np.pi) + log_std_x + (x-mu_x) ** 2 / log_std_x.exp())
        recon_loss = recon_loss.mean(dim=0).sum()
        kl_loss = utils.kl_divergence(y,torch.tensor(c.variance),mu_z,log_std_z)
        
        _, C, H, W = x.shape
        recon_loss /= (C * H * W)
        kl_loss /= (C * H * W)
        total_loss = recon_loss + c.beta * kl_loss

        return (total_loss, recon_loss, kl_loss)
    

    @torch.no_grad()
    def reconstruct(self, x):
        """
        Encode the input into the latent space and then decode it back from
        the latent representation. Return the reconstructed object.
        """
        mu_z, log_std_z = self.encoder(x)
        z = self.reparametrize(mu_z, log_std_z)
        mu_x, _ = self.decoder(z)
        return mu_x.cpu()

    @torch.no_grad()
    def sample(self, n):
        """
        Generate n samples using the decoder.
        """
        z = self.prior().sample((n, self.latent_dim))
        mu_x, log_std_x = self.decoder(z)
        return mu_x.cpu()
    
    def forward(self, x):
        """
        Perform forward pass through the network

        x: input image
        
        return: output image, mu and log_var vectors
        """
        mu, log_var = self.encoder(x)
        z = self.reparametrize(mu, log_var)
        output = self.decoder(z)

        return output, mu, log_var
    
    def predict_visual(self, x):
        """
        Get visual prediction

        x: input latent representation
        
        return: output image
        """
        
        input_ = torch.tensor(x, device=device, dtype=torch.float, requires_grad=True)
        output, _ = self.decoder(input_)

        return input_, output

    def predict_latent(self, x):
        """
        Get latent prediction
        
        x: input image
        
        return: output latent representation
        """
        input_ = torch.tensor(x, device=device, dtype=torch.float).unsqueeze(0)
        output, _ = self.encoder(input_)

        return output
    
    def get_grad(self, input_, output, error):
        """
        Get gradient with respect to prediction error

        input_: input tensor from the predict function
        output: output tensor from the predict function
        error: prediction error
        
        return: numpy array containing the gradient
        """
        # Set gradient to zero
        input_.grad = torch.zeros(input_.size(), device=device,
                                  dtype=torch.float, requires_grad=False)

        output.backward(torch.tensor(error, dtype=torch.float,
                                     device=device))

        return input_.grad.detach().cpu().squeeze().numpy()
    
    def save(self, path):
        """
        Save network to file
        """
        torch.save(self.state_dict(), path)

    def load(self, path):
        """
        Load network from file
        """
        self.load_state_dict(torch.load(path,
                                        map_location=device))
        self.eval()